x-llm-perf-common: &llm-perf-common
  build:
    dockerfile: Dockerfile.llmperf
  platform: linux/amd64
  shm_size: "2.4gb"
  networks:
    - performance-benchmark-network
  volumes:
    - ./config:/app/config:ro
    - ./results:/app/results:consistent
    - /var/run/docker.sock:/var/run/docker.sock
  group_add:
    - ${DOCKER_GID:-999}

services:
  otel_collector:
    profiles:
      - nekko_mode
    platform: linux/amd64
    image: otel/opentelemetry-collector:latest
    networks:
      - performance-benchmark-network
    ports:
      - "4317:4317"
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro

  nekko_api:
    profiles:
      - nekko_mode
    image: nekko-api:latest
    platform: linux/amd64
    build:
      context: ..
      dockerfile: docker/simple/Dockerfile
    cap_add:
      - SYS_RESOURCE
    volumes:
      - ./models:/app/models:ro
      - ./config/nekko_settings.json:/app/settings.json:ro
    networks:
      - performance-benchmark-network
    environment:
      - CONFIG_FILE=settings.json
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel_collector:4317
    ports:
      - "8000:8000"
    healthcheck:
      test: curl -f http://localhost:8000/v1/models
      interval: 30s
      timeout: 5s
      start_period: 10s
      retries: 5

  ollama_api:
    profiles:
      - ollama_mode
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama/models
      - ./scripts/ollama-setup.sh:/ollama-setup.sh:rw
    networks:
      - performance-benchmark-network
    entrypoint: ["/bin/bash", "-c", "ollama serve & until ollama list > /dev/null 2>&1; do sleep 5; done && /ollama-setup.sh && wait %1"]
    healthcheck:
      test: bash -c "ollama list | grep -q 'qwen2.5-3b-instruct-q5_k_m' || exit 1"
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 10s

  vllm_api:
    profiles:
      - vllm_mode
    image: vllm_cpu_image
    build:
      context: ./build/vllm
      dockerfile: Dockerfile.cpu
    platform: linux/amd64
    networks:
      - performance-benchmark-network
    environment:
      - VLLM_TARGET_DEVICE=cpu
      - OMP_NUM_THREADS=4
      - VLLM_CPU_KVCACHE_SPACE=8
      - VLLM_OPENVINO_CPU_KV_CACHE_PRECISION=u8
      - VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=1
    ports:
      - "8001:8001"
    command: [
      "--port", "8001",
      # Delete the comment if you want to test a large model similar to the one being tested on Neko. Change specs as well
      # "--model", "Qwen/Qwen2.5-3B-Instruct",
      "--dtype", "float32"
    ]
    healthcheck:
      test: curl -f http://localhost:8001/v1/models
      interval: 30s
      timeout: 5s
      start_period: 10s
      retries: 5

  llm_perf_nekko:
    <<: *llm-perf-common
    profiles:
      - nekko_mode
    depends_on:
      nekko_api:
        condition: service_healthy
    command: ["--api-names", "nekko"]

  llm_perf_vllm:
    <<: *llm-perf-common
    profiles:
      - vllm_mode
    depends_on:
      vllm_api:
        condition: service_healthy
    command: ["--api-names", "vllm"]

  llm_perf_ollama:
    <<: *llm-perf-common
    profiles:
      - ollama_mode
    depends_on:
      ollama_api:
        condition: service_healthy
    command: ["--api-names", "ollama"]
  
networks:
  performance-benchmark-network:
    driver: bridge
