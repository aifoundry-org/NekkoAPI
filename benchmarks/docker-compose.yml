# Common configuration for LLM performance containers
x-llm-perf-common: &llm-perf-common
  build:
    dockerfile: Dockerfile.llmperf
  platform: linux/amd64
  shm_size: "2.4gb"
  networks:
    - performance-benchmark-network
  volumes:
    - ./config:/app/config:ro
    - ./results:/app/results:consistent
    - ./scripts/combine_results.py:/combine_results.py:rw
    - /var/run/docker.sock:/var/run/docker.sock
  group_add:
    - ${DOCKER_GID:-999}

services:
  otel_collector:
    profiles:
      - nekko_mode
    platform: linux/amd64
    image: otel/opentelemetry-collector:latest
    networks:
      - performance-benchmark-network
    ports:
      - "4317:4317"
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro

  nekko_api:
    container_name: nekko_api
    profiles:
      - nekko_mode
    image: nekko-api:latest
    platform: linux/amd64
    build:
      context: ..
      dockerfile: docker/simple/Dockerfile
    cap_add:
      - SYS_RESOURCE
    volumes:
      - ./models:/app/models:ro
      - ./config/nekko_settings.json:/app/settings.json:ro
    networks:
      - performance-benchmark-network
    environment:
      - CONFIG_FILE=settings.json
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel_collector:4317
    ports:
      - "8000:8000"
    healthcheck:
      test: curl -f http://localhost:8000/v1/models
      interval: 30s
      timeout: 5s
      start_period: 10s
      retries: 5

  ollama_api:
    container_name: ollama_api
    profiles:
      - ollama_mode
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama/models
      - ./scripts/ollama-setup.sh:/ollama-setup.sh:rw
    networks:
      - performance-benchmark-network
    entrypoint: ["/bin/bash", "-c", "ollama serve & until ollama list > /dev/null 2>&1; do sleep 5; done && /ollama-setup.sh && wait %1"]
    healthcheck:
      test: bash -c "ollama list | grep -q 'qwen2.5-3b-instruct-q5_k_m' || exit 1"
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 10s

  vllm_api:
    container_name: vllm_api
    profiles:
      - vllm_mode
    image: vllm_cpu_image
    build:
      context: ./build/vllm
      dockerfile: Dockerfile.cpu
    platform: linux/amd64
    networks:
      - performance-benchmark-network
    environment:
      - VLLM_TARGET_DEVICE=cpu
      - OMP_NUM_THREADS=4
      - VLLM_CPU_KVCACHE_SPACE=8
      - VLLM_OPENVINO_CPU_KV_CACHE_PRECISION=u8
      - VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS=1
    ports:
      - "8001:8001"
    command: [
      "--port", "8001",
      # Commented out: For testing large models, adjust as needed:
      # "--model", "Qwen/Qwen2.5-3B-Instruct",
      "--dtype", "float32"
    ]
    healthcheck:
      test: curl -f http://localhost:8001/v1/models
      interval: 30s
      timeout: 5s
      start_period: 10s
      retries: 5

  tgi_api:
    container_name: tgi_api
    profiles:
      - tgi_mode
    image: tgi
    build:
      context: ./build/tgi
      dockerfile: Dockerfile_llamacpp
    platform: linux/amd64
    volumes:
      - ./models:/models
    networks:
      - performance-benchmark-network
    environment:
      - HF_HUB_ENABLE_HF_TRANSFER=0
    ports:
      - "8003:3000"
    command: [
      # If you have more than 10 cores, it may request too much RAM. Uncomment it
      # "--max-batch-size", "1",
      # "--max-total-tokens", "8192",
      "--model-id", "Qwen/Qwen2.5-3B-Instruct",
      "--model-gguf", "/models/qwen2.5-3b-instruct-q5_k_m.gguf"
    ]
    healthcheck:
      test: python3 -c 'import urllib.request,sys; sys.exit(0) if b"Qwen/Qwen2.5-3B-Instruct" in urllib.request.urlopen("http://localhost:3000/v1/models").read() else sys.exit(1)'
      interval: 30s
      timeout: 5s
      start_period: 10s
      retries: 5

  llm_perf_nekko:
    <<: *llm-perf-common
    profiles:
      - nekko_mode
    depends_on:
      nekko_api:
        condition: service_healthy
    command: ["sh", "-c", "python /app/collect_benchmarks.py --api-names nekko && python /combine_results.py"]

  llm_perf_vllm:
    <<: *llm-perf-common
    profiles:
      - vllm_mode
    depends_on:
      vllm_api:
        condition: service_healthy
    command: ["sh", "-c", "python /app/collect_benchmarks.py --api-names vllm && python /combine_results.py"]

  llm_perf_ollama:
    <<: *llm-perf-common
    profiles:
      - ollama_mode
    depends_on:
      ollama_api:
        condition: service_healthy
    command: ["sh", "-c", "python /app/collect_benchmarks.py --api-names ollama && python /combine_results.py"]

  llm_perf_tgi:
    <<: *llm-perf-common
    profiles:
      - tgi_mode
    depends_on:
      tgi_api:
        condition: service_healthy
    command: ["sh", "-c", "python /app/collect_benchmarks.py --api-names tgi && python /combine_results.py"]

networks:
  performance-benchmark-network:
    driver: bridge
